{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Preparacion de directorio"
      ],
      "metadata": {
        "id": "1-BCoOHP-Zzf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def setup_project(main_fold, project_fold, mount_drive=True):\n",
        "\n",
        "    \"\"\"\n",
        "    Mounts Google Drive, sets up sys.path, and changes directory to the project folder.\n",
        "    Only prints the current working directory and the project folder name.\n",
        "    \"\"\"\n",
        "    import os\n",
        "    import sys\n",
        "    from google.colab import drive\n",
        "\n",
        "    if mount_drive:\n",
        "        drive.mount('/content/drive')\n",
        "\n",
        "    base_dir = '/content/drive/MyDrive'\n",
        "    parent_project_dir = os.path.join(base_dir, main_fold)\n",
        "    project_dir = os.path.join(parent_project_dir, project_fold)\n",
        "\n",
        "    # Add parent directory to sys.path\n",
        "    if parent_project_dir not in sys.path:\n",
        "        sys.path.append(parent_project_dir)\n",
        "\n",
        "    # Change working directory if exists\n",
        "    if os.path.exists(project_dir):\n",
        "        os.chdir(project_dir)\n",
        "    else:\n",
        "        print(f\"Project directory not found: {project_dir}\")\n",
        "        return\n",
        "\n",
        "    # Add project directory to sys.path\n",
        "    if project_dir not in sys.path:\n",
        "        sys.path.append(project_dir)\n",
        "\n",
        "    # Print only the working directory and project folder name\n",
        "    print(f\"Current working directory: {os.getcwd()}\")\n",
        "    print(f\"Project folder: {project_fold}\")\n"
      ],
      "metadata": {
        "id": "Epk0cD8_8rvL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "main_fold = 'Machine Learning Projects'\n",
        "project_fold = 'Analisis Cluster Data Importacion'\n",
        "\n",
        "setup_project(main_fold, project_fold)"
      ],
      "metadata": {
        "id": "79lDamW28vN4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dependencias"
      ],
      "metadata": {
        "id": "NIJSSHZ6-e5Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import cupy as cp\n",
        "from cuml.manifold import UMAP as cuUMAP\n",
        "from cuml.cluster import HDBSCAN as cuHDBSCAN\n",
        "from bertopic.representation import OpenAI\n",
        "from bertopic import BERTopic\n",
        "import openai\n",
        "\n",
        "import re\n",
        "from lab_utils import get_data, tariff_description, sugerencias_builder"
      ],
      "metadata": {
        "id": "kB4kr0ES8xoI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "CARGA DE DATA"
      ],
      "metadata": {
        "id": "WvqqNSUS-hvF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ruta1 = './datos/data_importacion_sin_clasificar.csv'\n",
        "ruta2 = './datos/dicaranceles.xlsx'\n",
        "\n",
        "df= get_data(ruta1)\n",
        "df_diccionario = get_data(ruta2)\n",
        "\n",
        "shape1 = df.shape\n",
        "print(f'El dataframe tiene {shape1[0]} filas y {shape1[1]} columnas')\n"
      ],
      "metadata": {
        "id": "8Rl-crEP80NJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "MUESTREO Y AJUSTE DE DATA"
      ],
      "metadata": {
        "id": "tdbyjig0-oX-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.sample(n=10000, random_state=42)\n",
        "df.reset_index\n",
        "\n",
        "columna_codigo_arancelario = 'posicion_arancelaria'\n",
        "columna_descripcion = 'descripcion_x'\n",
        "df = tariff_description(df, df_diccionario, columna_codigo_arancelario)\n",
        "df = sugerencias_builder(df,columna_descripcion)"
      ],
      "metadata": {
        "id": "R8VeTeIR83Lx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "TRANSFORMACION Y TRATAMIENTO DE DATOS"
      ],
      "metadata": {
        "id": "4zrP1F3p-rIO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def corpus_cleaner(\n",
        "    df,\n",
        "    column_corpus: str = \"corpus\",\n",
        "    palabras_a_eliminar: list = None,\n",
        "    replacement_dict: dict = None,\n",
        "    new_column: str = \"corpus_limpio\"\n",
        "):\n",
        "    \"\"\"\n",
        "    Cleans a text column by removing unwanted words/stopwords and applying replacements.\n",
        "\n",
        "    Parameters:\n",
        "        df: DataFrame with the text column to clean.\n",
        "        column_corpus: Name of the column to clean.\n",
        "        palabras_a_eliminar: List of words to remove. If None, uses default list.\n",
        "        replacement_dict: Dictionary of replacements, e.g. {'tel√£fono': 'telefono'}.\n",
        "        new_column: Name of the column to store the cleaned output.\n",
        "\n",
        "    Returns:\n",
        "        DataFrame with the new cleaned column added.\n",
        "    \"\"\"\n",
        "    import re\n",
        "    import pandas as pd\n",
        "\n",
        "    if palabras_a_eliminar is None:\n",
        "        palabras_a_eliminar = [\n",
        "            # Art√≠culos\n",
        "            'el', 'la', 'los', 'las', 'un', 'una', 'unos', 'unas', 'lo', 'al', 'del',\n",
        "            # Preposiciones y conjunciones\n",
        "            'de', 'a', 'en', 'y', 'que', 'con', 'por', 'para', 'sin', 'sobre', 'bajo', 'entre', 'hacia', 'hasta', 'desde', 'durante',\n",
        "            # Pronombres y posesivos\n",
        "            'se', 'su', 'sus', 'le', 'les', 'me', 'te', 'nos', 'os', 'mi', 'tu', 'nuestro', 'vuestro',\n",
        "            # Adverbios y conectores redundantes\n",
        "            'muy', 'mas', 'menos', 'tambi√©n', 'adem√°s', 'incluso', 'inclusive', 'siempre', 'nunca', 'ahora', 'despu√©s', 'antes', 'aqu√≠', 'all√≠', 'pni√±a',\n",
        "            # Palabras gen√©ricas\n",
        "            'articulos', 'similares', 'partes', 'punto', 'tipo', 'vez', 'cosas', 'hecho', 'caso', 'ejemplo', 'manera', 'forma', 'fin', 'modo', 'demas', 'similar', 'otras',\n",
        "            'excepto',\t'partida', 'incluidas', 'incluidos', 'patrones', 'esta', 'clase', 'artic', 'partidas', 'aunque', 'esten', 'en', 'parte', 'mm', 'set', 'transporte',\n",
        "            # Verbos comunes\n",
        "            'ser', 'estar', 'tener', 'haber', 'hacer', 'poder', 'decir', 'ver', 'saber'\n",
        "        ]\n",
        "    if replacement_dict is None:\n",
        "        replacement_dict = {\n",
        "            'tel√£fono': 'telefono'\n",
        "            # Add more replacements here as needed\n",
        "        }\n",
        "\n",
        "    # Step 1: Remove punctuation\n",
        "    def clean_text(text):\n",
        "        if pd.isna(text):\n",
        "            return \"\"\n",
        "        text = re.sub(r'[^\\w\\s]', '', text)\n",
        "        # Build regex for exact words, case-insensitive\n",
        "        regex_palabras = r'(?<!\\w)(' + '|'.join(map(re.escape, palabras_a_eliminar)) + r')(?!\\w)'\n",
        "        text = re.sub(regex_palabras, '', text, flags=re.IGNORECASE)\n",
        "        text = re.sub(r'\\s+', ' ', text).strip()\n",
        "        # Remove numbers\n",
        "       # text = re.sub(r'\\d+', '', text)\n",
        "        return text\n",
        "\n",
        "    result = df.copy()\n",
        "    result[new_column] = result[column_corpus].apply(clean_text)\n",
        "\n",
        "    # Step 2: Replacements\n",
        "    for old_word, new_word in replacement_dict.items():\n",
        "        result[new_column] = result[new_column].str.replace(old_word, new_word, regex=False)\n",
        "\n",
        "    return result\n",
        "\n",
        "irrelevantes = set([\n",
        "    \"de\", \"las\", \"los\", \"y\", \"en\", \"a\", \"o\", \"del\", \"la\", \"el\", \"un\", \"una\", \"con\", \"por\", \"para\", \"al\"\n",
        "])\n",
        "\n",
        "keywords_simples = {\n",
        "    \"vehiculos\", \"veh√≠culo\", \"veh√≠culos\", \"automoviles\", \"autom√≥vil\", \"autom√≥viles\",\n",
        "    \"carro\", \"carros\", \"auto\", \"autos\", \"camioneta\", \"camionetas\", \"cami√≥n\", \"camiones\"\n",
        "}\n",
        "\n",
        "keywords_compuestas = [\n",
        "    \"repuesto para vehiculos\", \"repuesto para veh√≠culo\", \"repuesto para carro\", \"repuesto para auto\",\n",
        "    \"partes de carro\", \"partes de vehiculos\", \"partes de veh√≠culo\", \"partes de auto\", \"partes de camioneta\",\n",
        "    \"partes de cami√≥n\", \"accesorios de veh√≠culos\", \"accesorios de auto\", \"accesorios de carro\",\n",
        "    \"accesorios de camioneta\", \"accesorios de cami√≥n\", \"componentes automotrices\", \"industria automotriz\"\n",
        "]\n",
        "\n",
        "def get_first_two_words(text):\n",
        "    palabras = text.split()\n",
        "    return \" \".join(palabras[:2])\n",
        "\n",
        "def get_first_two_relevant(text):\n",
        "    palabras = re.split(r'\\W+', text.lower())\n",
        "    relevantes = [p for p in palabras if p and p not in irrelevantes]\n",
        "    return \" \".join(relevantes[:2])\n",
        "\n",
        "def keywords_en_corpus(corpus):\n",
        "    encontrados = set()\n",
        "    corpus_lower = corpus.lower()\n",
        "    for frase in keywords_compuestas:\n",
        "        if frase in corpus_lower:\n",
        "            encontrados.add(frase)\n",
        "    palabras = set(corpus_lower.split())\n",
        "    for kw in keywords_simples:\n",
        "        if kw in palabras:\n",
        "            encontrados.add(kw)\n",
        "    return \" \".join(encontrados)\n",
        "\n",
        "def patron_manufacturas(text2):\n",
        "    # Busca \"LAS DEMAS MANUFACTURAS DE [ART√çCULO]\"\n",
        "    match = re.search(r\"LAS DEMAS MANUFACTURAS DE ([\\w\\s]+)\", text2, re.IGNORECASE)\n",
        "    if match:\n",
        "        articulo = match.group(1).strip()\n",
        "        # Elimina stopwords y deja solo lo relevante\n",
        "        palabras = [w for w in articulo.split() if w.lower() not in irrelevantes]\n",
        "        if palabras:\n",
        "            return f\"manufacturas de {' '.join(palabras)}\"\n",
        "    return \"\"\n",
        "\n",
        "def patron_placas_laminas(text2):\n",
        "    # Busca \"PLACAS, LAMINAS, HOJAS Y TIRAS\"\n",
        "    if re.search(r\"PLACAS[, ]+LAMINAS\", text2, re.IGNORECASE):\n",
        "        return \"placas laminas\"\n",
        "    return \"\"\n",
        "\n",
        "def patron_manufacturas(text2):\n",
        "    # Busca \"LAS DEMAS MANUFACTURAS DE [ART√çCULO]\"\n",
        "    match = re.search(r\"LAS DEMAS MANUFACTURAS DE ([^,Y\\n]+)\", text2, re.IGNORECASE)\n",
        "    if match:\n",
        "        articulo = match.group(1).strip()\n",
        "        # Elimina stopwords y deja solo lo relevante\n",
        "        palabras = [w for w in articulo.split() if w.lower() not in irrelevantes]\n",
        "        if palabras:\n",
        "            return f\"manufacturas de {' '.join(palabras)}\"\n",
        "    return \"\"\n",
        "\n",
        "def extract_texto_lectura(row):\n",
        "    text2 = str(row[\"text2\"])\n",
        "    parte1 = get_first_two_words(str(row[\"corpus_limpio\"]))\n",
        "    parte2 = patron_manufacturas(text2)\n",
        "    parte3 = keywords_en_corpus(str(row[\"corpus_limpio\"]))\n",
        "    resultado = parte1\n",
        "    if parte2:\n",
        "        resultado = f\"{resultado} {parte2}\"\n",
        "    if parte3:\n",
        "        tokens_resultado = set(resultado.lower().split())\n",
        "        extra = \" \".join([x for x in parte3.split() if x not in tokens_resultado])\n",
        "        if extra:\n",
        "            resultado = f\"{resultado} {extra}\"\n",
        "    return resultado.strip().lower()"
      ],
      "metadata": {
        "id": "79CnqT5h84_0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = corpus_cleaner(df)\n",
        "\n",
        "# Aplica la funci√≥n al DataFrame\n",
        "df[\"texto_lectura\"] = df.apply(extract_texto_lectura, axis=1)\n",
        "\n",
        "abstracts = df['texto_lectura'].tolist()"
      ],
      "metadata": {
        "id": "I41ZxI7d88eR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "PREPARACION METOLOGIA BERTOPIC"
      ],
      "metadata": {
        "id": "Md7HHKqd-wji"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- CELDA 3: PREPARAR NLTK Y STOPWORDS ---\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "spanish_stopwords = set(stopwords.words('spanish'))\n",
        "\n",
        "# Si tienes un set de stopwords extra, define o importa aqu√≠:\n",
        "# Ejemplo:\n",
        "# EXTRA_STOPWORDS = {\"producto\", \"venta\", ...}\n",
        "try:\n",
        "    combined_stopwords = spanish_stopwords.union(EXTRA_STOPWORDS)\n",
        "except NameError:\n",
        "    print(\"EXTRA_STOPWORDS no definido, usando solo stopwords est√°ndar.\")\n",
        "    combined_stopwords = spanish_stopwords\n"
      ],
      "metadata": {
        "id": "LmfkDu3S9Ee8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- CELDA 7: VECTORIZADOR ---\n",
        "vectorizer_model = CountVectorizer(\n",
        "    stop_words=list(combined_stopwords),\n",
        "    min_df=2,\n",
        "    max_df=0.95,\n",
        "    ngram_range=(1, 2)\n",
        ")"
      ],
      "metadata": {
        "id": "8Detb5Or9H0l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- CELDA 8: MODELO DE REPRESENTACI√ìN OPENAI ---\n",
        "openai_api_key = \"sk-proj-...\"\n",
        "\n",
        "prompt = \"\"\"\n",
        "Tengo un conjunto de documentos, cada uno relacionado con productos:\n",
        "[DOCUMENTOS]\n",
        "Estos productos est√°n descritos por las siguientes palabras clave: [KEYWORDS]\n",
        "\n",
        "Tu tarea es extraer un nombre de categor√≠a corto y altamente descriptivo (m√°ximo 5 palabras) que represente al grupo de productos.\n",
        "No quiero un ‚Äút√≥pico‚Äù, ni un resumen, sino una palabra o frase representativa que funcione como categor√≠a, clase o etiqueta para el grupo de art√≠culos.\n",
        "Todos los documentos son productos, as√≠ que piensa en t√©rminos de categor√≠as comerciales.\n",
        "\n",
        "resultado, solo el nombre de la categoria. No incluyas nada m√°s\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "client = openai.OpenAI(api_key=openai_api_key)\n",
        "openai_model = OpenAI(client, model=\"gpt-3.5-turbo\", exponential_backoff=True, chat=True, prompt=prompt)\n",
        "representation_model = {\"OpenAI\": openai_model}\n"
      ],
      "metadata": {
        "id": "X8zx45SN9Jn9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Inicializar modelo\n",
        "#print(\"üîÑ Cargando modelo Qwen3...\")\n",
        "#embedding_model = SentenceTransformer(\"Qwen/Qwen3-Embedding-0.6B\")\n",
        "\n",
        "openai.api_key = openai_api_key\n",
        "\n",
        "\n",
        "print(\"‚úÖ Modelo cargado exitosamente\")\n",
        "\n",
        "# =====================================================\n",
        "# CONFIGURACI√ìN DE PROMPT\n",
        "# =====================================================\n",
        "custom_prompt = (\n",
        "\" The following are items descriptions in spanish so create 220 categories base on the function of the item :  \")\n",
        "prompts_alternatives = [\n",
        "    \"el siguiente texto son descripciones de productos, dame su categoria: \",\n",
        "    \"Clasifica este producto importado seg√∫n su categor√≠a comercial: \",\n",
        "    \"El siguiente es un producto de importaci√≥n, determina su categor√≠a: \",\n",
        "    \"Categor√≠a del producto: \",\n",
        "    \"Identifica la categor√≠a comercial de este art√≠culo: \",\n",
        "    \"¬øA qu√© categor√≠a pertenece este producto seg√∫n su uso?: \",\n",
        "    \"\",  # Sin prompt\n",
        "    \"Clasifica seg√∫n categor√≠as como electr√≥nicos, textiles, alimentos, etc.: \",\n",
        "    \"Bas√°ndose en la descripci√≥n, la categor√≠a de este producto es: \"\n",
        "]\n",
        "\n",
        "print(f\"üìù Prompt seleccionado: '{custom_prompt}'\")\n",
        "\n",
        "# Sup√≥n que abstracts es tu lista de descripciones\n",
        "texts = [custom_prompt + t for t in abstracts]\n",
        "\n",
        "# =====================================================\n",
        "# OBTENER EMBEDDINGS CON OPENAI\n",
        "# =====================================================\n",
        "def get_openai_embeddings(texts, model=\"text-embedding-3-large\"):\n",
        "    embeddings = []\n",
        "    batch_size = 100  # OpenAI permite hasta 2048 tokens por texto y hasta 2048 textos por request, pero lo conservador es 100\n",
        "    for i in range(0, len(texts), batch_size):\n",
        "        batch = texts[i:i+batch_size]\n",
        "        response = openai.embeddings.create(\n",
        "            input=batch,\n",
        "            model=model,\n",
        "        )\n",
        "        batch_embeddings = [d.embedding for d in response.data]\n",
        "        embeddings.extend(batch_embeddings)\n",
        "    return embeddings\n",
        "\n",
        "embeddings = get_openai_embeddings(texts)\n",
        "\n",
        "print(\"‚úÖ Embeddings generados con OpenAI\")\n"
      ],
      "metadata": {
        "id": "A_A7cipe9Vb3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalizar embeddings\n",
        "embeddings = np.array(embeddings)\n",
        "embeddings = embeddings / np.linalg.norm(embeddings, axis=1, keepdims=True)"
      ],
      "metadata": {
        "id": "516GKhIw9Ytk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# UMAP\n",
        "umap_model = cuUMAP(\n",
        "    n_neighbors=50,    # mayor es el numero mas macro es la clasificacion\n",
        "    n_components=500,  # a partir de 60 el modelo si hace buena generalizacion\n",
        "    min_dist=0.01,     # densidad de los clusters\n",
        "    metric='euclidean',\n",
        "    random_state=42,\n",
        "    verbose=True\n",
        ")\n",
        "reduced_embeddings = umap_model.fit_transform(embeddings)\n",
        "\n",
        "# HDBSCAN\n",
        "hdbscan_model = cuHDBSCAN(\n",
        "    min_cluster_size=12,\n",
        "    min_samples=7,\n",
        "    cluster_selection_epsilon=0.15,\n",
        "    metric='euclidean',\n",
        "    cluster_selection_method='eom',\n",
        "    prediction_data=True\n",
        ")"
      ],
      "metadata": {
        "id": "dtOob-og-Dn2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "GENERALIZAR CLASES"
      ],
      "metadata": {
        "id": "ejMywCZQ-12Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- CELDA 9: CREAR Y AJUSTAR BERTopic ---\n",
        "topic_model = BERTopic(\n",
        "    embedding_model=None,\n",
        "    umap_model=None,\n",
        "    hdbscan_model=hdbscan_model,\n",
        "    vectorizer_model=vectorizer_model,\n",
        "    representation_model=representation_model,\n",
        "    top_n_words=10,\n",
        "    verbose=True,\n",
        "    language=\"spanish\"\n",
        ")\n",
        "\n",
        "topics, probs = topic_model.fit_transform(abstracts, reduced_embeddings)"
      ],
      "metadata": {
        "id": "ZWoOJBbm9g3i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chatgpt_topic_labels = {topic: \" | \".join(list(zip(*values))[0]) for topic, values in topic_model.topic_aspects_[\"OpenAI\"].items()}\n",
        "chatgpt_topic_labels[-1] = \"Outlier Topic\"\n",
        "topic_model.set_topic_labels(chatgpt_topic_labels)"
      ],
      "metadata": {
        "id": "xKmzGM18-A2O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "EXPLORACION DE GRUPOS (SUBCATEGORIAS)"
      ],
      "metadata": {
        "id": "REFhxcWI-64e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "topic_model.get_topic(11, full=True)"
      ],
      "metadata": {
        "id": "QOAhm5AQ-EP6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "topic_5_docs_indices = [i for i, topic in enumerate(topics) if topic == 11]\n",
        "print(f\"Abstracts for Topic 5 ({len(topic_5_docs_indices)} documents):\")\n",
        "for i in topic_5_docs_indices:\n",
        "  print(f\"- {abstracts[i]}\")"
      ],
      "metadata": {
        "id": "yqIJIcd1-HVS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "topic_model.visualize_topics(custom_labels=True)"
      ],
      "metadata": {
        "id": "11dS3weP-Iuu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "CREACION DE CATEGORIAS"
      ],
      "metadata": {
        "id": "jXsYFtdj--Rl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import openai\n",
        "import ast\n",
        "import re\n",
        "\n",
        "def enrich_df_with_subcategory(df, topics, topic_model):\n",
        "    \"\"\"\n",
        "    A√±ade una columna de subcategor√≠a (nombre de t√≥pico) al DataFrame y devuelve el mapping topic_id_to_customname.\n",
        "    \"\"\"\n",
        "    # topic_model.get_topic_info() devuelve un DataFrame con Topic y Name\n",
        "    topic_info = topic_model.get_topic_info()\n",
        "    # Extract the custom names generated by OpenAI, defaulting to the BERTopic name if OpenAI is not available\n",
        "    if \"OpenAI\" in topic_model.topic_aspects_:\n",
        "         topic_id_to_customname = {topic: \" | \".join(list(zip(*values))[0]) for topic, values in topic_model.topic_aspects_[\"OpenAI\"].items()}\n",
        "    else:\n",
        "         topic_id_to_customname = dict(zip(topic_info.Topic, topic_info.Name))\n",
        "\n",
        "    # Ensure -1 is handled for outliers\n",
        "    topic_id_to_customname[-1] = \"Outlier Topic\"\n",
        "\n",
        "\n",
        "    df = df.copy()\n",
        "    df[\"Topic\"] = topics\n",
        "    # Use .get() with a default value for safety\n",
        "    df[\"subcategory\"] = df[\"Topic\"].map(topic_id_to_customname).fillna(\"Sin subcategor√≠a\")\n",
        "    return df, topic_id_to_customname\n",
        "\n",
        "def create_topic_to_category_mapping(topic_id_to_customname, openai_api_key, df, n_categories=None, prompt_extra=\"\"):\n",
        "    \"\"\"\n",
        "    Usa OpenAI para agrupar subcategor√≠as (por Topic) en categor√≠as principales de sentido com√∫n.\n",
        "    Si n_categories es None, el modelo decide el n√∫mero de categor√≠as.\n",
        "    \"\"\"\n",
        "    topics_in_data = set(df['Topic'].unique())\n",
        "    subcats = [f\"{k}: {v}\" for k, v in topic_id_to_customname.items() if k != -1]\n",
        "\n",
        "    if n_categories is not None:\n",
        "        prompt = (\n",
        "            f\"Quiero que agrupes las siguientes subcategor√≠as de productos en exactamente {n_categories} categor√≠as principales. Unas categorias que siempre deben existir es Electrodomesticos, Cocina, Muebles y Decoracion, Automotriz, Ferreteria, Deportes, Papeleria, Jugetes y Juegos.\"\n",
        "            \"Dame el resultado como un diccionario Python Topic: (category_code, category_name), donde category_code es un int y category_name un string. \"\n",
        "            \"Incluye absolutamente todos los Topic que te paso, incluyendo -1. \"\n",
        "            \"Si falta alguno, as√≠gnale (9999, 'Sin Categoria'). \"\n",
        "            \"El resultado debe estar SOLO contenido en un bloque de c√≥digo Python (triple backtick y python), sin ninguna explicaci√≥n, solo el diccionario.\\n\"\n",
        "            f\"Subcategor√≠as:\\n\" + \"\\n\".join(subcats) + f\"\\n{prompt_extra}\"\n",
        "        )\n",
        "    else:\n",
        "        prompt = (\n",
        "            \"Analiza la siguiente lista de subcategor√≠as de productos. Agr√∫palas en las categor√≠as principales m√°s naturales perosiempre deben existir las categorias Electrodomesticos, Cocina, Muebles y Decoracion, Automotriz, Ferreteria, Deportes, Papeleria, Jugetes y Juegos. estas son solo un baseline, lo demas hazlo a tu criterio\"\n",
        "            \"seg√∫n el significado y similitud de sus nombres. El n√∫mero de categor√≠as lo decides t√∫, seg√∫n lo que tenga m√°s sentido. \"\n",
        "            \"Por ejemplo, todo lo que sea ropa en una sola categor√≠a, igual con electr√≥nica, juguetes, etc. \"\n",
        "            \"Devu√©lveme solo un diccionario Python Topic: (category_code, category_name), donde category_code es un int √∫nico y category_name un string. \"\n",
        "            \"Incluye absolutamente todos los Topic que te paso, incluyendo -1. Si falta alguno, as√≠gnale (9999, 'Sin Categoria'). \"\n",
        "            \"El resultado debe estar SOLO contenido en un bloque de c√≥digo Python (triple backtick y python), sin ninguna explicaci√≥n, solo el diccionario.\\n\"\n",
        "            f\"Subcategor√≠as:\\n\" + \"\\n\".join(subcats) + f\"\\n{prompt_extra}\"\n",
        "        )\n",
        "\n",
        "    client = openai.OpenAI(api_key=openai_api_key)\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-4o\",\n",
        "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "        temperature=0.1,\n",
        "        max_tokens=5000,\n",
        "    )\n",
        "    content = response.choices[0].message.content\n",
        "\n",
        "    code_match = re.search(r\"```python\\s*\\n([\\s\\S]+?)```\", content)\n",
        "    if code_match:\n",
        "        mapping_str = code_match.group(1)\n",
        "    else:\n",
        "        code_match = re.search(r\"(\\{[\\s\\S]+\\})\", content)\n",
        "        if code_match:\n",
        "            mapping_str = code_match.group(1)\n",
        "        else:\n",
        "            raise ValueError(f\"No se pudo extraer el diccionario de la respuesta de OpenAI. Respuesta fue:\\n{content}\")\n",
        "\n",
        "    try:\n",
        "        topic_to_category = ast.literal_eval(mapping_str)\n",
        "    except Exception as e:\n",
        "        raise ValueError(f\"No se pudo interpretar el diccionario extra√≠do. String extra√≠do:\\n{mapping_str}\\nError: {str(e)}\")\n",
        "\n",
        "    for code in topics_in_data:\n",
        "        if code not in topic_to_category:\n",
        "            if code == -1:\n",
        "                topic_to_category[code] = (9999, \"Outlier Category\")\n",
        "            else:\n",
        "                topic_to_category[code] = (9999, \"Sin Categoria\")\n",
        "    return topic_to_category\n",
        "\n",
        "def enrich_df_with_category(df, topic_to_category):\n",
        "    \"\"\"\n",
        "    A√±ade columnas de c√≥digo de categor√≠a y nombre de categor√≠a al DataFrame usando el mapping topic_to_category.\n",
        "    \"\"\"\n",
        "    df = df.copy()\n",
        "    # Use .get() with a default value for safety\n",
        "    df[\"category_code\"] = df[\"Topic\"].map(lambda x: topic_to_category.get(x, (9999, \"Sin Categoria\"))[0])\n",
        "    df[\"category\"] = df[\"Topic\"].map(lambda x: topic_to_category.get(x, (9999, \"Sin Categoria\"))[1])\n",
        "    return df"
      ],
      "metadata": {
        "id": "c6d_vMt9-NtU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- PHASE 1: ORIGINAL DATA ---\n",
        "\n",
        "df, topic_id_to_customname = enrich_df_with_subcategory(df, topics, topic_model)\n",
        "\n",
        "# Ahora el modelo decide las categor√≠as seg√∫n sentido com√∫n (no pongas n_categories)\n",
        "topic_to_category = create_topic_to_category_mapping(\n",
        "    topic_id_to_customname,\n",
        "    openai_api_key,\n",
        "    df,\n",
        "    n_categories=None  # <--- Esto hace que se use el prompt de sentido com√∫n\n",
        ")\n",
        "\n",
        "df = enrich_df_with_category(df, topic_to_category)\n"
      ],
      "metadata": {
        "id": "M-mXllyL-Qav"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "GUARDAR DATOS ETIQUETAS CON CATEGORIA Y SUBCATEGORIA"
      ],
      "metadata": {
        "id": "rFmNf0pG_GB3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.to_csv('tablas/sample_OPENAI_2_SHOT.csv', index=False)"
      ],
      "metadata": {
        "id": "XREEhAVd-TKR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ANEXO : INSTALACION DE DEPENDENCIAS Y RAPIDS //"
      ],
      "metadata": {
        "id": "Rvw_GahK-Wmg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/rapidsai/rapidsai-csp-utils.git\n",
        "!python rapidsai-csp-utils/colab/pip-install.py\n",
        "!pip install bertopic\n",
        "!pip install umap-learn\n",
        "!pip install hdbscan"
      ],
      "metadata": {
        "id": "O-EcCNcU-WKU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}