{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Preparacion de directorio"
      ],
      "metadata": {
        "id": "1-BCoOHP-Zzf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def setup_project(main_fold, project_fold, mount_drive=True):\n",
        "\n",
        "    \"\"\"\n",
        "    Mounts Google Drive, sets up sys.path, and changes directory to the project folder.\n",
        "    Only prints the current working directory and the project folder name.\n",
        "    \"\"\"\n",
        "    import os\n",
        "    import sys\n",
        "    from google.colab import drive\n",
        "\n",
        "    if mount_drive:\n",
        "        drive.mount('/content/drive')\n",
        "\n",
        "    base_dir = '/content/drive/MyDrive'\n",
        "    parent_project_dir = os.path.join(base_dir, main_fold)\n",
        "    project_dir = os.path.join(parent_project_dir, project_fold)\n",
        "\n",
        "    # Add parent directory to sys.path\n",
        "    if parent_project_dir not in sys.path:\n",
        "        sys.path.append(parent_project_dir)\n",
        "\n",
        "    # Change working directory if exists\n",
        "    if os.path.exists(project_dir):\n",
        "        os.chdir(project_dir)\n",
        "    else:\n",
        "        print(f\"Project directory not found: {project_dir}\")\n",
        "        return\n",
        "\n",
        "    # Add project directory to sys.path\n",
        "    if project_dir not in sys.path:\n",
        "        sys.path.append(project_dir)\n",
        "\n",
        "    # Print only the working directory and project folder name\n",
        "    print(f\"Current working directory: {os.getcwd()}\")\n",
        "    print(f\"Project folder: {project_fold}\")\n"
      ],
      "metadata": {
        "id": "Epk0cD8_8rvL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "main_fold = 'Machine Learning Projects'\n",
        "project_fold = 'Analisis Cluster Data Importacion'\n",
        "\n",
        "setup_project(main_fold, project_fold)"
      ],
      "metadata": {
        "id": "79lDamW28vN4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dependencias"
      ],
      "metadata": {
        "id": "NIJSSHZ6-e5Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import cupy as cp\n",
        "from cuml.manifold import UMAP as cuUMAP\n",
        "from cuml.cluster import HDBSCAN as cuHDBSCAN\n",
        "from bertopic.representation import OpenAI\n",
        "from bertopic import BERTopic\n",
        "import openai\n",
        "\n",
        "import re\n",
        "from lab_utils import get_data, tariff_description, sugerencias_builder"
      ],
      "metadata": {
        "id": "kB4kr0ES8xoI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "CARGA DE DATA"
      ],
      "metadata": {
        "id": "WvqqNSUS-hvF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ruta1 = './datos/data_importacion_sin_clasificar.csv'\n",
        "ruta2 = './datos/dicaranceles.xlsx'\n",
        "\n",
        "df= get_data(ruta1)\n",
        "df_diccionario = get_data(ruta2)\n",
        "\n",
        "shape1 = df.shape\n",
        "print(f'El dataframe tiene {shape1[0]} filas y {shape1[1]} columnas')\n"
      ],
      "metadata": {
        "id": "8Rl-crEP80NJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "MUESTREO Y AJUSTE DE DATA"
      ],
      "metadata": {
        "id": "tdbyjig0-oX-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.sample(n=10000, random_state=42)\n",
        "df.reset_index\n",
        "\n",
        "columna_codigo_arancelario = 'posicion_arancelaria'\n",
        "columna_descripcion = 'descripcion_x'\n",
        "df = tariff_description(df, df_diccionario, columna_codigo_arancelario)\n",
        "df = sugerencias_builder(df,columna_descripcion)"
      ],
      "metadata": {
        "id": "R8VeTeIR83Lx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "TRANSFORMACION Y TRATAMIENTO DE DATOS"
      ],
      "metadata": {
        "id": "4zrP1F3p-rIO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def corpus_cleaner(\n",
        "    df,\n",
        "    column_corpus: str = \"corpus\",\n",
        "    palabras_a_eliminar: list = None,\n",
        "    replacement_dict: dict = None,\n",
        "    new_column: str = \"corpus_limpio\"\n",
        "):\n",
        "    \"\"\"\n",
        "    Cleans a text column by removing unwanted words/stopwords and applying replacements.\n",
        "\n",
        "    Parameters:\n",
        "        df: DataFrame with the text column to clean.\n",
        "        column_corpus: Name of the column to clean.\n",
        "        palabras_a_eliminar: List of words to remove. If None, uses default list.\n",
        "        replacement_dict: Dictionary of replacements, e.g. {'telãfono': 'telefono'}.\n",
        "        new_column: Name of the column to store the cleaned output.\n",
        "\n",
        "    Returns:\n",
        "        DataFrame with the new cleaned column added.\n",
        "    \"\"\"\n",
        "    import re\n",
        "    import pandas as pd\n",
        "\n",
        "    if palabras_a_eliminar is None:\n",
        "        palabras_a_eliminar = [\n",
        "            # Artículos\n",
        "            'el', 'la', 'los', 'las', 'un', 'una', 'unos', 'unas', 'lo', 'al', 'del',\n",
        "            # Preposiciones y conjunciones\n",
        "            'de', 'a', 'en', 'y', 'que', 'con', 'por', 'para', 'sin', 'sobre', 'bajo', 'entre', 'hacia', 'hasta', 'desde', 'durante',\n",
        "            # Pronombres y posesivos\n",
        "            'se', 'su', 'sus', 'le', 'les', 'me', 'te', 'nos', 'os', 'mi', 'tu', 'nuestro', 'vuestro',\n",
        "            # Adverbios y conectores redundantes\n",
        "            'muy', 'mas', 'menos', 'también', 'además', 'incluso', 'inclusive', 'siempre', 'nunca', 'ahora', 'después', 'antes', 'aquí', 'allí', 'pniña',\n",
        "            # Palabras genéricas\n",
        "            'articulos', 'similares', 'partes', 'punto', 'tipo', 'vez', 'cosas', 'hecho', 'caso', 'ejemplo', 'manera', 'forma', 'fin', 'modo', 'demas', 'similar', 'otras',\n",
        "            'excepto',\t'partida', 'incluidas', 'incluidos', 'patrones', 'esta', 'clase', 'artic', 'partidas', 'aunque', 'esten', 'en', 'parte', 'mm', 'set', 'transporte',\n",
        "            # Verbos comunes\n",
        "            'ser', 'estar', 'tener', 'haber', 'hacer', 'poder', 'decir', 'ver', 'saber'\n",
        "        ]\n",
        "    if replacement_dict is None:\n",
        "        replacement_dict = {\n",
        "            'telãfono': 'telefono'\n",
        "            # Add more replacements here as needed\n",
        "        }\n",
        "\n",
        "    # Step 1: Remove punctuation\n",
        "    def clean_text(text):\n",
        "        if pd.isna(text):\n",
        "            return \"\"\n",
        "        text = re.sub(r'[^\\w\\s]', '', text)\n",
        "        # Build regex for exact words, case-insensitive\n",
        "        regex_palabras = r'(?<!\\w)(' + '|'.join(map(re.escape, palabras_a_eliminar)) + r')(?!\\w)'\n",
        "        text = re.sub(regex_palabras, '', text, flags=re.IGNORECASE)\n",
        "        text = re.sub(r'\\s+', ' ', text).strip()\n",
        "        # Remove numbers\n",
        "       # text = re.sub(r'\\d+', '', text)\n",
        "        return text\n",
        "\n",
        "    result = df.copy()\n",
        "    result[new_column] = result[column_corpus].apply(clean_text)\n",
        "\n",
        "    # Step 2: Replacements\n",
        "    for old_word, new_word in replacement_dict.items():\n",
        "        result[new_column] = result[new_column].str.replace(old_word, new_word, regex=False)\n",
        "\n",
        "    return result\n",
        "\n",
        "irrelevantes = set([\n",
        "    \"de\", \"las\", \"los\", \"y\", \"en\", \"a\", \"o\", \"del\", \"la\", \"el\", \"un\", \"una\", \"con\", \"por\", \"para\", \"al\"\n",
        "])\n",
        "\n",
        "keywords_simples = {\n",
        "    \"vehiculos\", \"vehículo\", \"vehículos\", \"automoviles\", \"automóvil\", \"automóviles\",\n",
        "    \"carro\", \"carros\", \"auto\", \"autos\", \"camioneta\", \"camionetas\", \"camión\", \"camiones\"\n",
        "}\n",
        "\n",
        "keywords_compuestas = [\n",
        "    \"repuesto para vehiculos\", \"repuesto para vehículo\", \"repuesto para carro\", \"repuesto para auto\",\n",
        "    \"partes de carro\", \"partes de vehiculos\", \"partes de vehículo\", \"partes de auto\", \"partes de camioneta\",\n",
        "    \"partes de camión\", \"accesorios de vehículos\", \"accesorios de auto\", \"accesorios de carro\",\n",
        "    \"accesorios de camioneta\", \"accesorios de camión\", \"componentes automotrices\", \"industria automotriz\"\n",
        "]\n",
        "\n",
        "def get_first_two_words(text):\n",
        "    palabras = text.split()\n",
        "    return \" \".join(palabras[:2])\n",
        "\n",
        "def get_first_two_relevant(text):\n",
        "    palabras = re.split(r'\\W+', text.lower())\n",
        "    relevantes = [p for p in palabras if p and p not in irrelevantes]\n",
        "    return \" \".join(relevantes[:2])\n",
        "\n",
        "def keywords_en_corpus(corpus):\n",
        "    encontrados = set()\n",
        "    corpus_lower = corpus.lower()\n",
        "    for frase in keywords_compuestas:\n",
        "        if frase in corpus_lower:\n",
        "            encontrados.add(frase)\n",
        "    palabras = set(corpus_lower.split())\n",
        "    for kw in keywords_simples:\n",
        "        if kw in palabras:\n",
        "            encontrados.add(kw)\n",
        "    return \" \".join(encontrados)\n",
        "\n",
        "def patron_manufacturas(text2):\n",
        "    # Busca \"LAS DEMAS MANUFACTURAS DE [ARTÍCULO]\"\n",
        "    match = re.search(r\"LAS DEMAS MANUFACTURAS DE ([\\w\\s]+)\", text2, re.IGNORECASE)\n",
        "    if match:\n",
        "        articulo = match.group(1).strip()\n",
        "        # Elimina stopwords y deja solo lo relevante\n",
        "        palabras = [w for w in articulo.split() if w.lower() not in irrelevantes]\n",
        "        if palabras:\n",
        "            return f\"manufacturas de {' '.join(palabras)}\"\n",
        "    return \"\"\n",
        "\n",
        "def patron_placas_laminas(text2):\n",
        "    # Busca \"PLACAS, LAMINAS, HOJAS Y TIRAS\"\n",
        "    if re.search(r\"PLACAS[, ]+LAMINAS\", text2, re.IGNORECASE):\n",
        "        return \"placas laminas\"\n",
        "    return \"\"\n",
        "\n",
        "def patron_manufacturas(text2):\n",
        "    # Busca \"LAS DEMAS MANUFACTURAS DE [ARTÍCULO]\"\n",
        "    match = re.search(r\"LAS DEMAS MANUFACTURAS DE ([^,Y\\n]+)\", text2, re.IGNORECASE)\n",
        "    if match:\n",
        "        articulo = match.group(1).strip()\n",
        "        # Elimina stopwords y deja solo lo relevante\n",
        "        palabras = [w for w in articulo.split() if w.lower() not in irrelevantes]\n",
        "        if palabras:\n",
        "            return f\"manufacturas de {' '.join(palabras)}\"\n",
        "    return \"\"\n",
        "\n",
        "def extract_texto_lectura(row):\n",
        "    text2 = str(row[\"text2\"])\n",
        "    parte1 = get_first_two_words(str(row[\"corpus_limpio\"]))\n",
        "    parte2 = patron_manufacturas(text2)\n",
        "    parte3 = keywords_en_corpus(str(row[\"corpus_limpio\"]))\n",
        "    resultado = parte1\n",
        "    if parte2:\n",
        "        resultado = f\"{resultado} {parte2}\"\n",
        "    if parte3:\n",
        "        tokens_resultado = set(resultado.lower().split())\n",
        "        extra = \" \".join([x for x in parte3.split() if x not in tokens_resultado])\n",
        "        if extra:\n",
        "            resultado = f\"{resultado} {extra}\"\n",
        "    return resultado.strip().lower()"
      ],
      "metadata": {
        "id": "79CnqT5h84_0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = corpus_cleaner(df)\n",
        "\n",
        "# Aplica la función al DataFrame\n",
        "df[\"texto_lectura\"] = df.apply(extract_texto_lectura, axis=1)\n",
        "\n",
        "abstracts = df['texto_lectura'].tolist()"
      ],
      "metadata": {
        "id": "I41ZxI7d88eR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "PREPARACION METOLOGIA BERTOPIC"
      ],
      "metadata": {
        "id": "Md7HHKqd-wji"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- CELDA 3: PREPARAR NLTK Y STOPWORDS ---\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "spanish_stopwords = set(stopwords.words('spanish'))\n",
        "\n",
        "# Si tienes un set de stopwords extra, define o importa aquí:\n",
        "# Ejemplo:\n",
        "# EXTRA_STOPWORDS = {\"producto\", \"venta\", ...}\n",
        "try:\n",
        "    combined_stopwords = spanish_stopwords.union(EXTRA_STOPWORDS)\n",
        "except NameError:\n",
        "    print(\"EXTRA_STOPWORDS no definido, usando solo stopwords estándar.\")\n",
        "    combined_stopwords = spanish_stopwords\n"
      ],
      "metadata": {
        "id": "LmfkDu3S9Ee8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- CELDA 7: VECTORIZADOR ---\n",
        "vectorizer_model = CountVectorizer(\n",
        "    stop_words=list(combined_stopwords),\n",
        "    min_df=2,\n",
        "    max_df=0.95,\n",
        "    ngram_range=(1, 2)\n",
        ")"
      ],
      "metadata": {
        "id": "8Detb5Or9H0l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- CELDA 8: MODELO DE REPRESENTACIÓN OPENAI ---\n",
        "openai_api_key = \"sk-proj-...\"\n",
        "\n",
        "prompt = \"\"\"\n",
        "Tengo un conjunto de documentos, cada uno relacionado con productos:\n",
        "[DOCUMENTOS]\n",
        "Estos productos están descritos por las siguientes palabras clave: [KEYWORDS]\n",
        "\n",
        "Tu tarea es extraer un nombre de categoría corto y altamente descriptivo (máximo 5 palabras) que represente al grupo de productos.\n",
        "No quiero un “tópico”, ni un resumen, sino una palabra o frase representativa que funcione como categoría, clase o etiqueta para el grupo de artículos.\n",
        "Todos los documentos son productos, así que piensa en términos de categorías comerciales.\n",
        "\n",
        "resultado, solo el nombre de la categoria. No incluyas nada más\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "client = openai.OpenAI(api_key=openai_api_key)\n",
        "openai_model = OpenAI(client, model=\"gpt-3.5-turbo\", exponential_backoff=True, chat=True, prompt=prompt)\n",
        "representation_model = {\"OpenAI\": openai_model}\n"
      ],
      "metadata": {
        "id": "X8zx45SN9Jn9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Inicializar modelo\n",
        "#print(\"🔄 Cargando modelo Qwen3...\")\n",
        "#embedding_model = SentenceTransformer(\"Qwen/Qwen3-Embedding-0.6B\")\n",
        "\n",
        "openai.api_key = openai_api_key\n",
        "\n",
        "\n",
        "print(\"✅ Modelo cargado exitosamente\")\n",
        "\n",
        "# =====================================================\n",
        "# CONFIGURACIÓN DE PROMPT\n",
        "# =====================================================\n",
        "custom_prompt = (\n",
        "\" The following are items descriptions in spanish so create 220 categories base on the function of the item :  \")\n",
        "prompts_alternatives = [\n",
        "    \"el siguiente texto son descripciones de productos, dame su categoria: \",\n",
        "    \"Clasifica este producto importado según su categoría comercial: \",\n",
        "    \"El siguiente es un producto de importación, determina su categoría: \",\n",
        "    \"Categoría del producto: \",\n",
        "    \"Identifica la categoría comercial de este artículo: \",\n",
        "    \"¿A qué categoría pertenece este producto según su uso?: \",\n",
        "    \"\",  # Sin prompt\n",
        "    \"Clasifica según categorías como electrónicos, textiles, alimentos, etc.: \",\n",
        "    \"Basándose en la descripción, la categoría de este producto es: \"\n",
        "]\n",
        "\n",
        "print(f\"📝 Prompt seleccionado: '{custom_prompt}'\")\n",
        "\n",
        "# Supón que abstracts es tu lista de descripciones\n",
        "texts = [custom_prompt + t for t in abstracts]\n",
        "\n",
        "# =====================================================\n",
        "# OBTENER EMBEDDINGS CON OPENAI\n",
        "# =====================================================\n",
        "def get_openai_embeddings(texts, model=\"text-embedding-3-large\"):\n",
        "    embeddings = []\n",
        "    batch_size = 100  # OpenAI permite hasta 2048 tokens por texto y hasta 2048 textos por request, pero lo conservador es 100\n",
        "    for i in range(0, len(texts), batch_size):\n",
        "        batch = texts[i:i+batch_size]\n",
        "        response = openai.embeddings.create(\n",
        "            input=batch,\n",
        "            model=model,\n",
        "        )\n",
        "        batch_embeddings = [d.embedding for d in response.data]\n",
        "        embeddings.extend(batch_embeddings)\n",
        "    return embeddings\n",
        "\n",
        "embeddings = get_openai_embeddings(texts)\n",
        "\n",
        "print(\"✅ Embeddings generados con OpenAI\")\n"
      ],
      "metadata": {
        "id": "A_A7cipe9Vb3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalizar embeddings\n",
        "embeddings = np.array(embeddings)\n",
        "embeddings = embeddings / np.linalg.norm(embeddings, axis=1, keepdims=True)"
      ],
      "metadata": {
        "id": "516GKhIw9Ytk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# UMAP\n",
        "umap_model = cuUMAP(\n",
        "    n_neighbors=50,    # mayor es el numero mas macro es la clasificacion\n",
        "    n_components=500,  # a partir de 60 el modelo si hace buena generalizacion\n",
        "    min_dist=0.01,     # densidad de los clusters\n",
        "    metric='euclidean',\n",
        "    random_state=42,\n",
        "    verbose=True\n",
        ")\n",
        "reduced_embeddings = umap_model.fit_transform(embeddings)\n",
        "\n",
        "# HDBSCAN\n",
        "hdbscan_model = cuHDBSCAN(\n",
        "    min_cluster_size=12,\n",
        "    min_samples=7,\n",
        "    cluster_selection_epsilon=0.15,\n",
        "    metric='euclidean',\n",
        "    cluster_selection_method='eom',\n",
        "    prediction_data=True\n",
        ")"
      ],
      "metadata": {
        "id": "dtOob-og-Dn2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "GENERALIZAR CLASES"
      ],
      "metadata": {
        "id": "ejMywCZQ-12Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- CELDA 9: CREAR Y AJUSTAR BERTopic ---\n",
        "topic_model = BERTopic(\n",
        "    embedding_model=None,\n",
        "    umap_model=None,\n",
        "    hdbscan_model=hdbscan_model,\n",
        "    vectorizer_model=vectorizer_model,\n",
        "    representation_model=representation_model,\n",
        "    top_n_words=10,\n",
        "    verbose=True,\n",
        "    language=\"spanish\"\n",
        ")\n",
        "\n",
        "topics, probs = topic_model.fit_transform(abstracts, reduced_embeddings)"
      ],
      "metadata": {
        "id": "ZWoOJBbm9g3i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chatgpt_topic_labels = {topic: \" | \".join(list(zip(*values))[0]) for topic, values in topic_model.topic_aspects_[\"OpenAI\"].items()}\n",
        "chatgpt_topic_labels[-1] = \"Outlier Topic\"\n",
        "topic_model.set_topic_labels(chatgpt_topic_labels)"
      ],
      "metadata": {
        "id": "xKmzGM18-A2O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "EXPLORACION DE GRUPOS (SUBCATEGORIAS)"
      ],
      "metadata": {
        "id": "REFhxcWI-64e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "topic_model.get_topic(11, full=True)"
      ],
      "metadata": {
        "id": "QOAhm5AQ-EP6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "topic_5_docs_indices = [i for i, topic in enumerate(topics) if topic == 11]\n",
        "print(f\"Abstracts for Topic 5 ({len(topic_5_docs_indices)} documents):\")\n",
        "for i in topic_5_docs_indices:\n",
        "  print(f\"- {abstracts[i]}\")"
      ],
      "metadata": {
        "id": "yqIJIcd1-HVS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "topic_model.visualize_topics(custom_labels=True)"
      ],
      "metadata": {
        "id": "11dS3weP-Iuu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "CREACION DE CATEGORIAS"
      ],
      "metadata": {
        "id": "jXsYFtdj--Rl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import openai\n",
        "import ast\n",
        "import re\n",
        "\n",
        "def enrich_df_with_subcategory(df, topics, topic_model):\n",
        "    \"\"\"\n",
        "    Añade una columna de subcategoría (nombre de tópico) al DataFrame y devuelve el mapping topic_id_to_customname.\n",
        "    \"\"\"\n",
        "    # topic_model.get_topic_info() devuelve un DataFrame con Topic y Name\n",
        "    topic_info = topic_model.get_topic_info()\n",
        "    # Extract the custom names generated by OpenAI, defaulting to the BERTopic name if OpenAI is not available\n",
        "    if \"OpenAI\" in topic_model.topic_aspects_:\n",
        "         topic_id_to_customname = {topic: \" | \".join(list(zip(*values))[0]) for topic, values in topic_model.topic_aspects_[\"OpenAI\"].items()}\n",
        "    else:\n",
        "         topic_id_to_customname = dict(zip(topic_info.Topic, topic_info.Name))\n",
        "\n",
        "    # Ensure -1 is handled for outliers\n",
        "    topic_id_to_customname[-1] = \"Outlier Topic\"\n",
        "\n",
        "\n",
        "    df = df.copy()\n",
        "    df[\"Topic\"] = topics\n",
        "    # Use .get() with a default value for safety\n",
        "    df[\"subcategory\"] = df[\"Topic\"].map(topic_id_to_customname).fillna(\"Sin subcategoría\")\n",
        "    return df, topic_id_to_customname\n",
        "\n",
        "def create_topic_to_category_mapping(topic_id_to_customname, openai_api_key, df, n_categories=None, prompt_extra=\"\"):\n",
        "    \"\"\"\n",
        "    Usa OpenAI para agrupar subcategorías (por Topic) en categorías principales de sentido común.\n",
        "    Si n_categories es None, el modelo decide el número de categorías.\n",
        "    \"\"\"\n",
        "    topics_in_data = set(df['Topic'].unique())\n",
        "    subcats = [f\"{k}: {v}\" for k, v in topic_id_to_customname.items() if k != -1]\n",
        "\n",
        "    if n_categories is not None:\n",
        "        prompt = (\n",
        "            f\"Quiero que agrupes las siguientes subcategorías de productos en exactamente {n_categories} categorías principales. Unas categorias que siempre deben existir es Electrodomesticos, Cocina, Muebles y Decoracion, Automotriz, Ferreteria, Deportes, Papeleria, Jugetes y Juegos.\"\n",
        "            \"Dame el resultado como un diccionario Python Topic: (category_code, category_name), donde category_code es un int y category_name un string. \"\n",
        "            \"Incluye absolutamente todos los Topic que te paso, incluyendo -1. \"\n",
        "            \"Si falta alguno, asígnale (9999, 'Sin Categoria'). \"\n",
        "            \"El resultado debe estar SOLO contenido en un bloque de código Python (triple backtick y python), sin ninguna explicación, solo el diccionario.\\n\"\n",
        "            f\"Subcategorías:\\n\" + \"\\n\".join(subcats) + f\"\\n{prompt_extra}\"\n",
        "        )\n",
        "    else:\n",
        "        prompt = (\n",
        "            \"Analiza la siguiente lista de subcategorías de productos. Agrúpalas en las categorías principales más naturales perosiempre deben existir las categorias Electrodomesticos, Cocina, Muebles y Decoracion, Automotriz, Ferreteria, Deportes, Papeleria, Jugetes y Juegos. estas son solo un baseline, lo demas hazlo a tu criterio\"\n",
        "            \"según el significado y similitud de sus nombres. El número de categorías lo decides tú, según lo que tenga más sentido. \"\n",
        "            \"Por ejemplo, todo lo que sea ropa en una sola categoría, igual con electrónica, juguetes, etc. \"\n",
        "            \"Devuélveme solo un diccionario Python Topic: (category_code, category_name), donde category_code es un int único y category_name un string. \"\n",
        "            \"Incluye absolutamente todos los Topic que te paso, incluyendo -1. Si falta alguno, asígnale (9999, 'Sin Categoria'). \"\n",
        "            \"El resultado debe estar SOLO contenido en un bloque de código Python (triple backtick y python), sin ninguna explicación, solo el diccionario.\\n\"\n",
        "            f\"Subcategorías:\\n\" + \"\\n\".join(subcats) + f\"\\n{prompt_extra}\"\n",
        "        )\n",
        "\n",
        "    client = openai.OpenAI(api_key=openai_api_key)\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-4o\",\n",
        "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "        temperature=0.1,\n",
        "        max_tokens=5000,\n",
        "    )\n",
        "    content = response.choices[0].message.content\n",
        "\n",
        "    code_match = re.search(r\"```python\\s*\\n([\\s\\S]+?)```\", content)\n",
        "    if code_match:\n",
        "        mapping_str = code_match.group(1)\n",
        "    else:\n",
        "        code_match = re.search(r\"(\\{[\\s\\S]+\\})\", content)\n",
        "        if code_match:\n",
        "            mapping_str = code_match.group(1)\n",
        "        else:\n",
        "            raise ValueError(f\"No se pudo extraer el diccionario de la respuesta de OpenAI. Respuesta fue:\\n{content}\")\n",
        "\n",
        "    try:\n",
        "        topic_to_category = ast.literal_eval(mapping_str)\n",
        "    except Exception as e:\n",
        "        raise ValueError(f\"No se pudo interpretar el diccionario extraído. String extraído:\\n{mapping_str}\\nError: {str(e)}\")\n",
        "\n",
        "    for code in topics_in_data:\n",
        "        if code not in topic_to_category:\n",
        "            if code == -1:\n",
        "                topic_to_category[code] = (9999, \"Outlier Category\")\n",
        "            else:\n",
        "                topic_to_category[code] = (9999, \"Sin Categoria\")\n",
        "    return topic_to_category\n",
        "\n",
        "def enrich_df_with_category(df, topic_to_category):\n",
        "    \"\"\"\n",
        "    Añade columnas de código de categoría y nombre de categoría al DataFrame usando el mapping topic_to_category.\n",
        "    \"\"\"\n",
        "    df = df.copy()\n",
        "    # Use .get() with a default value for safety\n",
        "    df[\"category_code\"] = df[\"Topic\"].map(lambda x: topic_to_category.get(x, (9999, \"Sin Categoria\"))[0])\n",
        "    df[\"category\"] = df[\"Topic\"].map(lambda x: topic_to_category.get(x, (9999, \"Sin Categoria\"))[1])\n",
        "    return df"
      ],
      "metadata": {
        "id": "c6d_vMt9-NtU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- PHASE 1: ORIGINAL DATA ---\n",
        "\n",
        "df, topic_id_to_customname = enrich_df_with_subcategory(df, topics, topic_model)\n",
        "\n",
        "# Ahora el modelo decide las categorías según sentido común (no pongas n_categories)\n",
        "topic_to_category = create_topic_to_category_mapping(\n",
        "    topic_id_to_customname,\n",
        "    openai_api_key,\n",
        "    df,\n",
        "    n_categories=None  # <--- Esto hace que se use el prompt de sentido común\n",
        ")\n",
        "\n",
        "df = enrich_df_with_category(df, topic_to_category)\n"
      ],
      "metadata": {
        "id": "M-mXllyL-Qav"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "GUARDAR DATOS ETIQUETAS CON CATEGORIA Y SUBCATEGORIA"
      ],
      "metadata": {
        "id": "rFmNf0pG_GB3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.to_csv('tablas/sample_OPENAI_2_SHOT.csv', index=False)"
      ],
      "metadata": {
        "id": "XREEhAVd-TKR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ANEXO : INSTALACION DE DEPENDENCIAS Y RAPIDS //"
      ],
      "metadata": {
        "id": "Rvw_GahK-Wmg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/rapidsai/rapidsai-csp-utils.git\n",
        "!python rapidsai-csp-utils/colab/pip-install.py\n",
        "!pip install bertopic\n",
        "!pip install umap-learn\n",
        "!pip install hdbscan"
      ],
      "metadata": {
        "id": "O-EcCNcU-WKU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}